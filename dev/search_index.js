var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"api/#MarginalLogDensities.Cubature","page":"API Reference","title":"MarginalLogDensities.Cubature","text":"Cubature([; solver=LBFGS(), adtype=AutoForwardDiff(), upper=nothing, lower=nothing, nσ=6; opt_func_kwargs...])\n\nConstruct a Cubature marginalizer to integrate out marginal variables via numerical integration (a.k.a. cubature).\n\nIf explicit upper and lower bounds for the integration are not supplied, this marginalizer will attempt to select good ones by first optimizing the marginal variables, doing a  Laplace approximation at their mode, and then going nσ standard deviations away on either side, assuming approximate normality.\n\nThe integration is performed using hcubature from Cubature.jl.\n\nArguments\n\nsolver=LBFGS() : Algorithm to use when performing the inner optimization to find the\n\nmode of the marginalized variables. Can be any algorithm defined in Optim.jl.\n\nadtype=AutoForwardDiff() : Automatic differentiation type to use for the \n\ninner optimization. AutoForwardDiff() is robust and fast for small problems; for larger ones AutoReverseDiff() or AutoZygote() are likely better.\n\nupper, lower : Optional upper and lower bounds for the numerical integration. If\n\nsupplied, they must be numeric vectors the same length as the marginal variables.\n\nnσ=6.0 : If upper and lower are not supplied, integrate this many standard\n\ndeviations away from the mode based on a Laplace approximation to the curvature at that  point.\n\nopt_func_kwargs : Optional keyword arguments passed on to\n\nOptimization.OptimizationFunction.\n\n\n\n\n\n","category":"type"},{"location":"api/#MarginalLogDensities.LaplaceApprox","page":"API Reference","title":"MarginalLogDensities.LaplaceApprox","text":"LaplaceApprox([solver=LBFGS() [; adtype=AutoForwardDiff(), opt_func_kwargs...]])\n\nConstruct a LaplaceApprox marginalizer to integrate out marginal variables via the Laplace approximation. This method will usually be faster than Cubature, especially in high dimensions, though it may not be as accurate.\n\nArguments\n\nsolver=LBFGS() : Algorithm to use when performing the inner optimization to find the\n\nmode of the marginalized variables. Can be any algorithm defined in Optim.jl.\n\nadtype=AutoForwardDiff() : Automatic differentiation type to use for the inner\n\noptimization, specified via the ADTypes.jl interface. AutoForwardDiff() is robust and fast for small problems; for larger ones AutoReverseDiff() or AutoZygote() are likely better.\n\nopt_func_kwargs : Optional keyword arguments passed on to\n\nOptimization.OptimizationFunction.\n\n\n\n\n\n","category":"type"},{"location":"api/#MarginalLogDensities.MarginalLogDensity","page":"API Reference","title":"MarginalLogDensities.MarginalLogDensity","text":"MarginalLogDensity(logdensity, u, iw, data, [method=LaplaceApprox(); \n[hess_adtype=nothing, sparsity_detector=DenseSparsityDetector(method.adtype, atol=cbrt(eps())),\ncoloring_algorithm=GreedyColoringAlgorithm()]])\n\nConstruct a callable object which wraps the function logdensity and integrates over a subset of its arguments.\n\nThe resulting MarginalLogDensity object  mld can then be called like a function as mld(v, data), where v is the subset of the full parameter vector u which is not indexed by iw.  If length(u) == n and length(iw) == m, then length(v) == n-m.\n\nArguments\n\nlogdensity : function with signature (u, data) returning a positive\n\nlog-probability (e.g. a log-pdf, log-likelihood, or log-posterior). In this function, u is a vector of variable parameters and data is an object (Array, NamedTuple, or whatever) that contains data and/or fixed parameters.\n\nu : Vector of initial values for the parameter vector.\niw : Vector of indices indicating which elements of u should be marginalized.\ndata=() : Optional argument\nmethod : How to perform the marginalization.  Defaults to LaplaceApprox(); Cubature()\n\nis also available.\n\nhess_adtype = nothing : Specifies how to calculate the Hessian of the marginalized \n\nvariables. If not specified, defaults to a sparse second-order method using forward AD  over the AD type given in the method (AutoForwardDiff() is the default).  Other backends can be set by loading the appropriate AD package and using the ADTypes.jl  interface.\n\nsparsity_detector = DenseSparsityDetector(method.adtype, atol=cbrt(eps)) : How to\n\nperform the sparsity detection. Detecting sparsity takes some time and may not be worth it for small problems, but for larger problems it can be extremely worth it. The default  DenseSparsityDetector is most robust, but if it's too slow, or if you're running out of  memory on a larger problem, try the tracing-based dectectors from SparseConnectivityTracer.jl.\n\ncoloring_algorithm = GreedyColoringAlgorithm() : How to determine the matrix \"colors\"\n\nto compress the sparse Hessian.\n\nExamples\n\njulia> using MarginalLogDensities, Distributions\n\njulia> N = 4\n\njulia> dist = MvNormal(I(3))\n\njulia> data = (N=N, dist=dist)\n\njulia> function logdensity(u, data) # arbitrary simple density function\n           return logpdf(data.dist, u) \n       end\n\njulia> u0 = rand(N)\n\njulia> mld = MarginalLogDensity(logdensity, u0, [1, 3], data)\n\njulia> mld(rand(2), data)\n\n\n\n\n\n\n","category":"type"},{"location":"api/#MarginalLogDensities.cached_hessian-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.cached_hessian","text":"Get the value of the cached Hessian matrix.\n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.cached_joint-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.cached_joint","text":"Get the values of the joint parameters v as of the last time mld was called. Equivalent to `cached_params(mld)[mld.iv]\n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.cached_marginal-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.cached_marginal","text":"Get the value of the cached marginal variables w, conditional on the values of the joint parameters v the last time mld was called. Equivalent to  cached_params(mld)[mld.iw]\n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.cached_params-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.cached_params","text":"Get the value of the cached parameter vector u. This includes the latest values given  for the non-marginalized variables v, as well as the modal values of the marginalized variables w conditional on v.\n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.ijoint-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.ijoint","text":"Return the indices of the non-marginalized variables, iv, in u \n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.imarginal-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.imarginal","text":"Return the indices of the marginalized variables, iw, in u \n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.merge_parameters-Union{Tuple{T2}, Tuple{T1}, Tuple{AbstractVector{T1}, AbstractVector{T2}, Any, Any, Any}} where {T1, T2}","page":"API Reference","title":"MarginalLogDensities.merge_parameters","text":"Splice together the estimated (fixed) parameters v and marginalized (random) parameters w into the single parameter vector u, based on their indices iv and iw.\n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.nfull-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.nfull","text":"Return the full dimension of the marginalized function, i.e. length(u) \n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.njoint-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.njoint","text":"Return the number of non-marginalized variables.\n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.nmarginal-Tuple{MarginalLogDensity}","page":"API Reference","title":"MarginalLogDensities.nmarginal","text":"Return the number of marginalized variables.\n\n\n\n\n\n","category":"method"},{"location":"api/#MarginalLogDensities.split_parameters-Tuple{Any, Any, Any}","page":"API Reference","title":"MarginalLogDensities.split_parameters","text":"Split the vector of all parameters u into its estimated (fixed) components v and marginalized (random) components w, based on their indices iv and iw. components\n\n\n\n\n\n","category":"method"},{"location":"sparse_ad/#Configuring-sparse-AD","page":"Sparsity and AD","title":"Configuring sparse AD","text":"","category":"section"},{"location":"theory/#How-it-works","page":"Theory","title":"How it works","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"By default, this package uses Laplace's method to approximate this integral. The Laplace approximation is fast in high dimensions, and works well for log-densities that are approximately Gaussian. The marginalization can also be done via numerical integration, a.k.a. cubature, which may be more accurate but will not scale well for higher dimensional problems. You can choose which marginalizer to use by passing the appropriate AbstractMarginalizer to MarginalLogDensity:","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"MarginalLogDensity(logdensity, u, iw, data, Cubature())\nMarginalLogDensity(logdensity, u, iw, data, LaplaceApprox())","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Both Cubature() and LaplaceApprox() can be specified with various options; refer to their  docstrings for details.","category":"page"},{"location":"turing/#Turing.jl-integration","page":"Turing integration","title":"Turing.jl integration","text":"","category":"section"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"Starting with Turing v0.40.4  / DynamicPPL v0.37.4, it is possible to construct a MarginalLogDensity directly from a Turing model,  specifying the variables to marginalize by their names.","category":"page"},{"location":"turing/#Model-definition","page":"Turing integration","title":"Model definition","text":"","category":"section"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"Models can be defined using Turing's @model macro and probabilistic programming syntax.  Here, we define a simple model with a latent variable x that follows a Gaussian  distribution with mean m and covariance C.","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"using Turing\nusing MarginalLogDensities\nusing DynamicPPL\nusing Distributions\nusing LinearAlgebra\nusing StatsPlots\nusing Random\n\nRandom.seed!(4321)\n\nr = 0.4\na = 2\nn = 50\nm = fill(a, n)\nC = Tridiagonal(fill(-r, n-1), ones(n), fill(-r, n-1))\ny = rand(MvNormal(m, C))\nplot(y)\n\n@model function demo(y)\n    n = length(y)\n    r ~ Uniform(-0.5, 0.5)\n    a ~ Normal(0, 5)\n    m ~ MvNormal(fill(a, n), 1)\n    C = Tridiagonal(fill(-r, n-1), ones(n), fill(-r, n-1))\n\n    y ~ MvNormal(m, C)\nend\n\nfull = demo(y)\nmarginal = marginalize(full, [@varname(m)],\n    sparsity_detector=DenseSparsityDetector(AutoForwardDiff(), atol=1e-9))\n\nnjoint(marginal)\nmarginal([0.1, 1.0])","category":"page"},{"location":"turing/#Maximum-a-posteriori-optimization","page":"Turing integration","title":"Maximum a-posteriori optimization","text":"","category":"section"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"using Optimization, OptimizationOptimJL\n\nv0 = zeros(2)\nopt_func = OptimizationFunction(marginal)\nopt_prob = OptimizationProblem(opt_func, v0, ())\nopt_sol = solve(opt_prob, NelderMead())","category":"page"},{"location":"turing/#MCMC-Sampling","page":"Turing integration","title":"MCMC Sampling","text":"","category":"section"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"Sampling from a MarginalLogDensity is currently a bit more awkward than doing so from a Turing model, but not too bad. Keep in mind that MarginalLogDensity objects don't currently work with AD, so samplers must either be gradient free (like random-walk Metropolis Hastings), or use a finite-difference backend (e.g. AutoFiniteDiff().)","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"The code below was adapted from @torfjelde's example on GitHub here. We  request 100 samples with a thinning rate of 20 - that is, we'll run 2,000 samples and keep  every 20th one. We also set the inital parameters to the MAP optimum we found before,  which speeds up convergence in this case.","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"using AbstractMCMC, AdvancedMH\n\nsampler = AdvancedMH.RWMH(njoint(marginal))\nchain = sample(marginal, sampler, 100; chain_type=MCMCChains.Chains,\n    thinning=20, initial_params=opt_sol.u,\n    # HACK: this a dirty way to extract the variable names in a model; it won't work in general.\n    param_names=setdiff(keys(DynamicPPL.untyped_varinfo(full)), [@varname(m)])\n)\nplot(chain)","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"These chains are short for the sake of the demo, but could easily be run longer to get a  smoother posterior.","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"Sampling using Hamiltonian Monte Carlo is possible, but is currently a bit more awkward. The following is adapted from the AdvancedHMC  documentation:","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"using AdvancedHMC\nimport FiniteDiff\nusing LogDensityProblems\n\n# Set the number of samples to draw and warmup iterations\nn_samples, n_adapts = 200, 100\n\n# Define a Hamiltonian system\nmetric = DiagEuclideanMetric(njoint(marginal))\nhamiltonian = Hamiltonian(metric, marginal, AutoFiniteDiff())\ninitial_ϵ = 0.4 \nintegrator = Leapfrog(initial_ϵ)\nkernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n\nsamples, stats = sample(\n    hamiltonian, kernel, opt_sol.u, n_samples, adaptor, n_adapts; progress=true,\n)\nplot(hcat(samples...)', layout=(2,  1), xlabel=\"Iteration\", ylabel=[\"r\" \"a\"], legend=false)","category":"page"},{"location":"turing/#Un-linking-parameters","page":"Turing integration","title":"Un-linking parameters","text":"","category":"section"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"One of the nice things about using Turing is that the DynamicPPL modeling language handles all the variable transformations, so that optimization and sampling can take place in  unconstrained parameter space even when you have bounded parameters (e.g. r in this  example). By default, marginalize sets up the MarginalLogDensity to use these unconstrained (a.k.a. \"linked\", to use the language familiar from generalized linear modeling) parameters. ","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"If you want transform them back to unlinked space, i.e. how they appear inside the model,  you need to construct a VarInfo and query it like this:","category":"page"},{"location":"turing/","page":"Turing integration","title":"Turing integration","text":"vi = VarInfo(marginal, opt_sol.u)\nvi[@varname(r)]","category":"page"},{"location":"hreg/#A-more-realistic-example:-hierarchical-regression","page":"Hierarchical regression","title":"A more realistic example: hierarchical regression","text":"","category":"section"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"Let's show how to use MLD on a slightly more complex problem, similar to one you might actually encounter in practice: a hierarchical linear regression. Our response variable  y is a linear function of the predictor x plus some normally-distributed noise.  Our data are divided into 50 different groups, and each group i has its own intercept term a_i. These intercepts are in turn drawn from a normal distribution with mean mu_0 and standard deviation sigma_0.","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"beginaligned\nmu_i j = a_i + b x_ij \ny_ij sim mathrmNormal(mu_i j sigma) \na_i sim mathrmNormal(mu_0 sigma_0)\nendaligned","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"Where i indexes the categories and j indexes the individual data points.","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"Choosing values for these parameters and simulating a dataset yields the following plot:","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"using MarginalLogDensities\nusing Distributions\nusing StatsPlots\nusing Random\n\nRandom.seed!(123)\nncategories = 50\npoints_per_category = 5\ncategories = 1:ncategories\nμ0 = 5.0\nσ0 = 5.0\naa = rand(Normal(μ0, σ0), ncategories)\nb = 4.5\nσ = 1.5\ncategory = repeat(categories, inner=points_per_category)\nn = length(category)\nx = rand(Uniform(-1, 1), n)\nμ = [aa[category[i]] + b * x[i] for i in 1:n]\ny = rand.(Normal.(μ, σ))\n\nscatter(x, y, color=category, legend=false, xlabel=\"x\", ylabel=\"y\")","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"Given this model structure, we can write a function for the log-likelihood of the data,  conditional on a vector of parameters u. The function is written so that u is  unbounded, that is, there are no constraints on any of its elements. This means it needs to include exp transformations for the elements of u corresponding toσ0 and σ to make sure they end up non-negative inside the model.","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"function loglik(u, data)\n    # unpack the parameters\n    μ0 = u[1]\n    σ0 = exp(u[2])\n    σ = exp(u[3])\n    b = u[4]\n    aa = u[5:end]\n    # predict the data\n    μ = [aa[data.category[i]] + b * data.x[i] for i in 1:data.n]\n    # calculate and return the log-likelihood\n    return loglikelihood(Normal(μ0, σ0), aa) + sum(logpdf.(Normal.(μ, σ), data.y))\nend\n\ndata = (; x, y, category, n)\nu0 = randn(4 + ncategories) # μ0, σ0, σ, b, and aa\nloglik(u0, data)","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"Say that we're not particularly interested in the individual intercepts a_i, but want to do inference on the other parameters. One way to approach this is to marginalize them","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"# select variables of interest out of complete parameter vector\niv = 1:4 \nv0 = u0[iv]\n\n# indices of nuisance parameters: everything esle \niw = setdiff(eachindex(u0), iv)\n\n# construct a MarginalLogDensity\nmld = MarginalLogDensity(loglik, u0, iw, data)","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"We can then call mld like a function:","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"mld(v0)","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"And, using Optimization.jl, estimate the maximum marginal-likelihood values of our four parameters of interest:","category":"page"},{"location":"hreg/","page":"Hierarchical regression","title":"Hierarchical regression","text":"using Optimization, OptimizationOptimJL\n\nopt_func = OptimizationFunction(mld)\nopt_prob = OptimizationProblem(opt_func, v0, data)\nopt_sol = solve(opt_prob, NelderMead())\n\nμ0_opt, logσ0_opt, logσ_opt, b_opt = opt_sol.u\n\nprintln(\"Estimated μ0:  $(round(μ0_opt, digits=2))  (true value $(μ0))\")\nprintln(\"Estimated σ0:  $(round(exp(logσ0_opt), digits=2))  (true value $(σ0))\")\nprintln(\"Estimated b:   $(round(b_opt, digits=2))  (true value $(b))\")\nprintln(\"Estimated σ:   $(round(exp(logσ_opt), digits=2))   (true value $(σ))\")","category":"page"},{"location":"componentarrays/#Making-life-easier-with-ComponentArrays","page":"Using ComponentArrays","title":"Making life easier with ComponentArrays","text":"","category":"section"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"MarginalLogDensities requires you to write your log-density as a function of a single flat array of parameters, u. This makes the internal marginalization calculations easier to perform, but as your model becomes more complex, it becomes more annoying and error-prone to keep track of which indices in u refer to which variables inside your model. One  solution to this problem is provided in ComponentArrays.jl, from the SciML ecosystem.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"A ComponentArray is a type that behaves like an ordinary Array, but organized into  blocks that can be accessed via named indices:","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"kjulia> using ComponentArrays\n\njulia> u = ComponentArray(a = 5.0, b = [-0.1, 4.8])\nComponentVector{Float64}(a = 5.0, b = [-0.1, 4.8])\n\njulia> u.a\n5.0\n\njulia> u.b\n2-element view(::Vector{Float64}, 2:3) with eltype Float64:\n -0.1\n  4.8\n\njulia> ones(3,3) * u # behaves like a normal vector\n3-element Vector{Float64}:\n 9.7\n 9.7\n 9.7","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"If you define your parameters as a ComponentVector, working with them inside your log-density function becomes much easier, without introducing any computational overhead.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"MarginalLogDensities works with ComponentVectors as well: instead of specifying the  integer indices of the variables to marginalize, you can give Symbols referring to  blocks of the parameter vector.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"To illustrate this, we'll fit a state-space model to some simulated time-series data. Specifically, we will have a two-dimensional vector autoregressive process, where the  hidden state mathbfx evolves according to ","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"beginaligned\nmathbfx_t = A mathbfx_t-1 + mathbfepsilon_t \nmathbfepsilon sim mathrmMvNormal(0 b^2 I)\nendaligned","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"where A is a square matrix and mathbfepsilon is the process noise, with standard  deviation b. Out data, mathbfy_t, are centered on mathbfy_t, plus some  Gaussian noise with standard deviation c.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"mathbfy_t sim mathrmMvNormal(mathbfx_t c^2 I)","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"We'll assume we know c a priori, but would like to estimate the transition matrix A  and the process noise b, while integrating out the unobserved states mathbfx_t for  all times t. ","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"Simulating and plotting our time series and observations:","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"using MarginalLogDensities\nusing ComponentArrays\nusing Distributions\nimport ReverseDiff\nusing Optimization, OptimizationOptimJL\nusing StatsPlots\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(1234)\n\nA = [0.8 -0.2; \n    -0.1  0.5]\nb = 0.3\nc = 0.1\n\nx = zeros(2, n)\nfor i in 2:n\n    x[:, i] = A * x[:, i-1] + b * randn(2)\nend\ny = x .+ c .* randn.()\n\nplot(x', layout=(2,1), legend=false, ylabel=[\"x1\" \"x2\"], xlabel=[\"\" \"Time\"])\nscatter!(y', markersize=2, markerstrokewidth=0)","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"Next, we define our log-density function. Note how we unpack the parameters using dot- notation, e.g. log_b = u.log_b (where log_b is a scalar) and A = u.A (where A is a vector).","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"The parameters for the variances b and c are supplied in the log-domain and exp- transformed to make sure they're always positive inside the model.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"\nfunction logdensity(u, data)\n    A = u.A\n    b = exp(u.log_b)\n    x  = u.x\n\n    y = data.y\n    n = data.n\n    c  = data.c\n\n    # Prior on initial state\n    ll = logpdf(MvNormal(zeros(2), 10), x[:, 1])\n    # first observation\n    ll += logpdf(MvNormal(x[:, 1], c), y[:, 1])\n    # rest of time series\n    for t in 2:n\n        ll += logpdf(MvNormal(A * x[:, t-1], b), x[:, t])\n        ll += logpdf(MvNormal(x[:, t], c), y[:, t])\n    end\n    return ll\nend","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"The inital value for our parameter vector is constructed as a ComponentArray to make it compatible with logdensity as it's written. The fixed data are a NamedTuple. These  are passed to the MarginalLogDensity constructor along with the function, as usual.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"u0 = ComponentArray(\n    A = A,\n    log_b = 0,\n    x = ones(2, n)\n)\ndata = (y = y, n = n, c = c)\njoint_vars = [:A, :log_b]\n\nmld = MarginalLogDensity(logdensity, u0, [:x], data,\n    LaplaceApprox(adtype=AutoReverseDiff()), sparsity_detector=TracerSparsityDetector())","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"However, we've specified we want to integrate out the latent states x as a  random effect: we just pass a Vector containing the symbol(s) of the variables to  marginalize.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"note: Note\nWe specify the ReverseDiff AD backend to use for gradients inside the LaplaceApproximation method. Since our marginal variables are fairly high dimensional  (a 1D latent state times 300 time steps gives 600 marginal parameters), reverse-mode AD is much faster than the default ForwardDiff backend. See the page on sparse automatic  differentiation for more info.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"From here, we select the subset of u0 containing the non-marginal variables, set up an optimization problem based on mld, and solve it.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"joint_vars = [:A, :log_b]\nv0 = u0[joint_vars]\nfunc = OptimizationFunction(mld);\nprob = OptimizationProblem(func, v0, data)\nsol = solve(prob, NelderMead())","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"We can use the delta method (based on finite differences) to estimate standard errors for our time-series parameters.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"estimates = sol.u\nstd_err = 1.96 ./ sqrt.(diag(hessian(v -> -mld(v), AutoFiniteDiff(), sol.u)))\nscatter(Vector(sol.u), yerr = std_err, label=\"Fitted\",\n    xticks=(1:5, [\"A[1,1]\", \"A[1,2]\", \"A[2,1]\", \"A[2,2]\", \"log_b\"]))\nscatter!([vec(A); log(b)], label=\"True value\")","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"The plot shows that they match up fairly well with the true values.","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"We can also access the latest value of the latent state, conditional on these point  estimates:","category":"page"},{"location":"componentarrays/","page":"Using ComponentArrays","title":"Using ComponentArrays","text":"mld(sol.u)\nx_hat = cached_params(mld).x \nx_err = 1.96 ./ sqrt.(diag(cached_hessian(mld)))\n\nplot(x_hat', ribbon=x_err', label=\"Estimated state ± 2 s.d.\", layout=(2, 1), color=3)\nplot!(x', label=\"Latent state\", xlabel=\"Time step\", color=1)\nscatter!(y', label=\"Noisy observations\", markerstrokewidth=0, markersize=2, color=2)","category":"page"},{"location":"#MarginalLogDensities.jl","page":"Getting started","title":"MarginalLogDensities.jl","text":"","category":"section"},{"location":"#Introduction","page":"Getting started","title":"Introduction","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"This package implements tools for integrating out (marginalizing) parameters from log-probability functions, such as likelihoods and posteriors of statistical models. This approach is useful for fitting models that incorporate random effects or  latent variables that are important for structuring the model, but whose exact  values may not be of interest in and of themselves. In the language of regression,  these are known as \"random effects,\" and in other contexts, they may be called \"nuisance parameters.\" Whatever they are called, we hope that our log-density function can be optimized faster and/or more reliably if we focus on the parameters we are actually interested in, averaging the log-density over all possible values of  the ones we are not.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"warning: Warning\nThis package is under development and should still be considered experimental at this point. Exercise appropriate caution and check your results if you're using it for  anything important. Bug reports, pull requests, and new use-cases are all welcome!","category":"page"},{"location":"#Installation","page":"Getting started","title":"Installation","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"MarginalLogDensities (MLD for short) requires Julia v1.10 or greater. It is a registered Julia package, so you can install it from the REPL by typing ] to enter package-manager mode, then running","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"pkg> add MarginalLogDensities","category":"page"},{"location":"#Basic-tutorial","page":"Getting started","title":"Basic tutorial","text":"","category":"section"},{"location":"","page":"Getting started","title":"Getting started","text":"A basic example follows. We start out by writing a function for our target log- density. This function must have the signature f(u, data), where u is a numeric vector of parameters and data contains any data, or fixed parameters, needed by the function.  The data argument can be anything you'd like, such as a Vector, NamedTuple, DataFrame, or some other struct. (If your function does not depend on any external data, just ignore that argument in the function body)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nNote that the convention for this package is to write the function as a positive log-density.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using MarginalLogDensities, Distributions, LinearAlgebra\n\nN = 3\nσ = 1.5\ndist = MvNormal(σ^2 * I(3))\ndata = (N=N, dist=dist)\n\nfunction logdensity(u, data)\n   return logpdf(data.dist, u) \nend","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"We then set up a marginalized version of this function. First, we set an initial value for our parameter vector u, as well as a set of indices iw indicating the subset of u that we want to integrate out. ","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"u = [1.1, -0.3, 0.1] # arbitrary initial values\niw = [1, 3]","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"We create the marginalized version of the function by calling the MarginalLogDensity constructor with the original function logdensity, the full parameter vector u, the indices of the marginalized values iw, and the data. (If your function does not depend on the data argument, it can be omitted here.)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"marginal_logdensity = MarginalLogDensity(logdensity, u, iw, data)","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Here, we are saying that we want to calculate logdensity as a function of u[2] only,  while integrating over all possible values of u[1] and u[3]. In the laguage of  mathematics, if we write u -> logdensity(u, data) as f(u), then the marginalized  function v -> marginal_logdensity(v, data) is calculating","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"f_m(u_2) = iint f(u)  du_1 du_3","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"After defining marginal_logdensity, you can call it just like the original function, with the difference that you only need to supply v, the subset of parameters you're  interested in, rather than the entire set u. ","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"initial_v = [1.0] # another arbitrary starting value\nlength(initial_v) == N - length(iw) # true\nmarginal_logdensity(initial_v, data)\n# -1.5466258635350596","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"Compare this value to the analytic marginal density, i.e. the log-probability of N(0 15) at 1.0:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"logpdf(Normal(0, 1.5), 1.0)\n# -1.5466258635350594","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"You also can re-run the same MarginalLogDensity with different data if you want -  though if the sparsity of your problem depends on the data in shome way, this may cause errors. See the section on sparsity for more information.","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"The point of doing all this was to find an optimal set of parameters v for your data. MLD includes an interface to Optimization.jl that  works directly with a MarginalLogDensity object, making  optimization easy. The simplest way is to construct an OptimizationProblem directly from the MarginalLogDensity and solve it:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"using Optimization, OptimizationOptimJL\n\nopt_problem = OptimizationProblem(marginal_logdensity, v0)\nopt_solution = solve(opt_problem, NelderMead())","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"If you want more control over options, for instance setting an AD backend, you can construct an OptimizationFunction explicitly:","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"opt_function = OptimizationFunction(marginal_logdensity, AutoFiniteDiff())\nopt_problem = OptimizationProblem(opt_function, v0)\nopt_solution = solve(opt_problem, LBFGS())","category":"page"},{"location":"","page":"Getting started","title":"Getting started","text":"note: Note\nAt present we can't differentiate through the Laplace approximation, so outer  optimizations like this need to either use a gradient-free solver (like NelderMead()), or a finite-difference backend (like AutoFiniteDiff()). We hope to remove this  limitation in the future.","category":"page"},{"location":"tips/#Tips-for-success","page":"Tips for success","title":"Tips for success","text":"","category":"section"}]
}
